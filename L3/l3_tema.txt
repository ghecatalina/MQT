#se poate folosi docker-compose-ul din L2, cel cu 1 broker
#we can use docker-compose file from Mosule2 
1. Create topics

Create  topic events1 with 1 replication factor and 3 partitions.
/usr/bin/kafka-topics --create --bootstrap-server kafka:9092 --replication-factor 1 --partitions 3 --topic events1
Create  topic events2 with 1 replication factor and 4 partitions.
/usr/bin/kafka-topics --create --bootstrap-server kafka:9092 --replication-factor 1 --partitions 4 --topic events2

2. List all topics 
/usr/bin/kafka-topics --list --bootstrap-server kafka:9092

3. Describe topic 
/usr/bin/kafka-topics --bootstrap-server kafka:9092 --describe --topic events1

4. Create a console producer foe topic event1. 
/usr/bin/kafka-console-producer --bootstrap-server kafka:9092 --topic events1
3. Read the data - create 2 consumers for event1. Show partition number and offset. 
docker exec -ti kafka /usr/bin/kafka-console-consumer --bootstrap-server kafka:9092 --topic events2 --property print.key=true --property key.separator="," --property print.partition=true --property print.offset=true

docker exec -ti kafka /usr/bin/kafka-console-consumer --bootstrap-server kafka:9092 --topic events1 --property print.partition=true --property print.offset=true --from-beginning

4. Send data. Use your Producer API
SimpleExampleProducer
SynchronousSimpleProducer
AsynchronousSimpleProducer
ExampleProducer --> add arg[0] parameter
create group of Consumers 

5. Run with specifying consumer group and printing the partition
- from-beginning
-latest

Check that messages with the same key go to the same partition. Notice, that messages may come in a different order, when they are in different partitions.

6. Delete topic
/usr/bin/kafka-topics --bootstrap-server kafka:9092 --topic events1 --delete
/usr/bin/kafka-topics --bootstrap-server kafka:9092 --topic events2 --delete



